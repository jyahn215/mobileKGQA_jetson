


load_model = {
    # "llm": LLM,
    # "inference_llm": LLM,
    # "pt_llm": PromptTuningLLM,
}

llama_model_path = {
    "7b": "meta-llama/Llama-2-7b-hf",
    "8b": "meta-llama/Meta-Llama-3-8B",
    "7b_chat": "meta-llama/Llama-3-7b-chat-hf",
    "13b": "meta-llama/Llama-3-13b-hf",
    "13b_chat": "meta-llama/Llama-3-13b-chat-hf",
}
